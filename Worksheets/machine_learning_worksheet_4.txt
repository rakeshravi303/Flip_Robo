WORKSHEET
MACHINE LEARNING – WORKSHEET 4
In Q1 to Q8, only one option is correct, Choose the correct option:

1. Which of the following in sklearn library is used for hyper parameter tuning?
A) GridSearchCV() B) RandomizedCV()
C) K-fold Cross Validation D) None of the above
Ans:- A

2. In which of the below ensemble techniques trees are trained in parallel?
A) Random forest B) Adaboost
C) Gradient Boosting D) All of the above
Ans:- A

3. In machine learning, if in the below line of code:
sklearn.svm.SVC (C=1.0, kernel='rbf', degree=3)
we increasing the C hyper parameter, what will happen?
A) The regularization will increase B) The regularization will decrease
C) No effect on regularization D) kernel will be changed to linear
Ans:- B

4. Check the below line of code and answer the following questions:
sklearn.tree.DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2)
Which of the following is true regarding max_depth hyper parameter?
A) It regularizes the decision tree by limiting the maximum depth up to which a tree can be grown.
B) It denotes the number of children a node can have.
C) both A & B
D) None of the above
Ans:- C

5. Which of the following is true regarding Random Forests?
A) It's an ensemble of weak learners.
B) The component trees are trained in series
C) In case of classification problem, the prediction is made by taking mode of the class labels predicted by the component trees.
D)None of the above
Ans:- A

6. What can be the disadvantage if the learning rate is very high in gradient descent?
A) Gradient Descent algorithm can diverge from the optimal solution.
B) Gradient Descent algorithm can keep oscillating around the optimal solution and may not settle.
C) Both of them
D)None of them.
Ans:- C

7. As the model complexity increases, what will happen?
A) Bias will increase, Variance decrease B) Bias will decrease, Variance increase
C)both bias and variance increase D) Both bias and variance decrease.
Ans:- B

8. Suppose I have a linear regression model which is performing as follows:
Train accuracy=0.95
Test accuracy=0.75
Which of the following is true regarding the model?
A) model is underfitting B) model is overfitting
C) model is performing good D) None of the above
Ans:- B

Q9 to Q15 are subjective answer type questions, Answer them briefly.

9. Suppose we have a dataset which have two classes A and B. The percentage of class A is 40% and percentage of class B is 60%. Calculate the Gini index and entropy of the dataset.
Ans:-
    
Gini Index= 1-[(40/100)^2+(60/100)^2]= 0.48
Entropy = -[(40/100) * log2 (40/100)] –[(60/100) *log2 (60/100)]= 0.971

10. What are the advantages of Random Forests over Decision Tree?
Ans:-
Decision trees have a low bias / are non-parametric, they suffer from a high variance which makes them less useful for most practical applications.
By aggregating multiple decision trees, one can reduce the variance of the model output significantly, thus improving performance. While this could be archived by simple tree bagging, the fact that each tree is build on a bootstrap sample of the same data gives a lower bound on the variance reduction, due to correlation between the individual trees. Random Forest addresses this problem by sub-sampling features, thus de-correlating the trees to a certain extend and therefore allowing for a greater variance reduction / increase in performance.

11. What is the need of scaling all numerical features in a dataset? Name any two techniques used for scaling.
Ans:-
11)	Scaling all numerical features in a dataset is used to treat skewed features and rescale them for modelling, when all features are in the same scale, it also helps algorithms to understand the relative relationship better. 
Two techniques used for scaling are,
1) Standard Scaler
2) Min-Max Scaler

12. Write down some advantages which scaling provides in optimization using gradient descent algorithm.
Ans:-
Gradient descent which is an optimization algorithm often used in Logistic Regression, SVM, Neural Networks etc. is another prominent example where if features are on different scale, certain weights are updated faster than others. However, feature scaling helps in causing Gradient Descent to converge much faster as standardizing all the variables on to the same scale

13. In case of a highly imbalanced dataset for a classification problem, is accuracy a good metric to measure the performance of the model. If not, why?
Ans:-
In a problem where there is a large class imbalance, a model can predict the value of the majority class for all predictions and achieve a high classification accuracy. So, further performance measures are needed such as Confusion matrix and ROC-AUC Score along with accuracy.

14. What is “f-score" metric? Write its mathematical formula.
Ans:-
The F-score, also called the F1-score, is a measure of a model’s accuracy on a dataset. It is used to evaluate binary classification systems, which classify examples into ‘positive’ or ‘negative’. F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. 
The formula for the F1 score is: F1 = 2 * (precision * recall) / (precision + recall) 

15. What is the difference between fit(), transform() and fit_transform()?
Ans:-
fit()computes the mean and std to be used for later scaling. 
transform() uses a previously computed mean and std to autoscale the data (subtract mean from all values and then divide it by std).
fit_transform() does both at the same time. So you can do it with 1 line of code instead of 2.

