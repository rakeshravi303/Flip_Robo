MACHINE LEARNING WORKSHEET – 2

In Q1 to Q5, only one option is correct, Choose the correct option:
1. In which of the following you can say that the model is overfitting?
A) High R-squared value for train-set and High R-squared value for test-set.
B) Low R-squared value for train-set and High R-squared value for test-set.
C) High R-squared value for train-set and Low R-squared value for test-set.
D) None of the above
Ans:- A

2. Which among the following is a disadvantage of decision trees?
A) Decision trees are prone to outliers.
B) Decision trees are highly prone to overfitting.
C) Decision trees are not easy to interpret
D) None of the above.
Ans:- A

3. Which of the following is an ensemble technique?
 A) SVM B) Logistic Regression
C) Random Forest D) Decision tree
Ans:- C

4. Suppose you are building a classification model for detection of a fatal disease where detection of the disease is most
important. In this case which of the following metrics you would focus on?
A) Accuracy B) Sensitivity
C) Precision D) None of the above.
Ans:- B

5. The value of AUC (Area under Curve) value for ROC curve of model A is 0.70 and of model B is 0.85. Which of these two
models is doing better job in classification?
A) Model A B) Model B
C) both are performing equal D) Data Insufficient
Ans:- B

In Q6 to Q9, more than one options are correct, Choose all the correct options:
6. Which of the following are the regularization technique in Linear Regression??
A) Ridge B) R-squared
 C) MSE D) Lasso
Ans:- B, C

7. Which of the following is not an example of boosting technique?
 A) Adaboost B) Decision Tree
C) Random Forest D) Xgboost.
Ans:- B, C

8. Which of the techniques are used for regularization of Decision Trees?
A) Pruning B) L2 regularization
C) Restricting the max depth of the tree D) All of the above
Ans:- D

9. Which of the following statements is true regarding the Adaboost technique?
A) We initialize the probabilities of the distribution as 1/n, where n is the number of data-points
B) A tree in the ensemble focuses more on the data points on which the previous tree was not performing well
C) It is example of bagging technique
D) None of the above
Ans:- A, B

Q10 to Q15 are subjective answer type questions, Answer them briefly.
10. Explain how does the adjusted R-squared penalize the presence of unnecessary predictors in the model?
Ans:- 
The adjusted R-squared will penalize for adding independent variables that do not fit the model. In regression analysis, it can be tempting to add more variables to the data as you think of them. Some of those variables will be significant, but you can’t be sure that significance is just by chance. The adjusted R2 will compensate for this by that penalizing for those extra variables.

11. Differentiate between Ridge and Lasso Regression.
Ans:-
A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression. The key difference between these techniques is that Lasso shrinks the less important feature’s coefficient to zero thus, removing some feature altogether. So, this works well for feature selection in case we have a huge number of features. 
In ridge regression, the penalty is the sum of the squares of the coefficients and for the Lasso, it’s the sum of the absolute values of the coefficients. It’s a shrinkage towards zero using an absolute value (l1 penalty) rather than a sum of squares(l2 penalty).

Ridge regression can’t zero coefficients. Here, you either select all the coefficients or none of them whereas LASSO does both parameter shrinkage and variable selection automatically because it zero out the co-efficients of collinear variables. Here it helps to select the variable(s) out of given n variables while performing lasso regression.

Ridge is computationally less intensive than Lasso.
 
12. What is VIF? What is the suitable value of a VIF for a feature to be included in a regression modelling?
Ans:- 
VIF stands for Variance Inflation Factor. During regression analysis, VIF assesses whether factors are correlated to each other (multicollinearity), which could affect p-values and the model isn't going to be as reliable.
There are some guidelines we can use to determine whether our VIFs are in an acceptable range. A rule of thumb commonly used in practice is if a VIF is > 10, you have high multicollinearity. In our case, with values around 1, we are in good shape, and can proceed with our regression

13. Why do we need to scale the data before feeding it to the train the model?
Ans:-
Scaling is a step of Data Pre Processing which is applied to independent variables or features of data. It basically helps to normalise the data within a particular range. Sometimes, it also helps in speeding up the calculations in an algorithm.

14. What are the different metrics which are used to check the goodness of fit in linear regression?
Ans:-
R-squared, the overall F-test, and the Root Mean Square Error (RMSE). 

15. From the following confusion matrix calculate sensitivity, specificity, precision, recall and accuracy.
Actual/Predicted True False
True 1000 50
False 250 1200
Ans:-

Sensitivity (SN) is calculated as the number of correct positive predictions divided by the total number of positives. It is also called recall (REC) or true positive rate (TPR). The best sensitivity is 1.0, whereas the worst is 0.0.
Sensitivity = TP / (TP + FN) = 1000 / (1000+50) = 0.95

Specificity (SP) is calculated as the number of correct negative predictions divided by the total number of negatives. It is also called true negative rate (TNR). The best specificity is 1.0, whereas the worst is 0.0.
Specificity = TN / (TN + FP) = 1200/(1200+250) = 0.82

Precision is calculated as the number of correct positive predictions (TP) divided by the total number of positive predictions (TP + FP).
Precision = TP / (TP + FP)
precision = 1000 / (1000 + 250) = 0.8

Accuracy (ACC) is calculated as the number of all correct predictions divided by the total number of the dataset. The best accuracy is 1.0, whereas the worst is 0.0. It can also be calculated by 1 – ERR.
Accuracy = TP+TN / (P + N) = (1000+1200) / (1000+50+250+1200) = 0.88