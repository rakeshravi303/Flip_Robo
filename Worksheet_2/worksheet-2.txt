In Q1 to Q9, only one option is correct, Choose the correct option:
1.	Which of the following extracts information from user generated content?
A) Java script tagging			B) Web scraping
C) A/B testing				D) MROCs
Ans: B
2.	Which of the following is not a web scraping library in python?
A) selenium				B) Beautiful soup
C) Requests				C) scrapy
Ans: C
3.	Selenium tests __________?
A) Browser based applications		B) DOS applications
C) GUI applications			D) All of the above
Ans: A
4.	Task of crawling is performed by a complex software which is known as:
A) Scraper				B) Crawler
C) Boat					D) Spider
Ans: B
5.	Which of the following commands is used to access name of a tag in Beautiful Soup?
A) tag.attrs				B) tag.name
C) tag,id				C) tag[‘id’]
Ans: B
6.	Which of the following is the default parser in Beautiful Soup?
A) html.parser				B) html5lib
C) lxml					D) lxml-xml
Ans: C
7.	In selenium the webdriver is used to?
A) design a test using selenese
B) test a web application on firefox only
C) execute tests on HtmlUnit browser
D) to download any content from a webpage
Ans: D
8.	In selenium, driver.find_elements_by_xpath(‘given xpath’) returns:
A) the first webelement associated with the ‘given xpath’
B) the url of first webelement associated with the ‘given xpath’
C) the list of all webelements associated with the ‘given xpath’
D) all the attributes of the first webelement associated with the ‘given xpath’
Ans: C
9.	The script ‘window.scrollBy(0,a) scrolls the webpage by?
A) ‘a’ number of horizontal spaces
B) ‘a’ number of lines
C) ‘a’ number of pixels horizontally
D) ‘a’ number of pixels vertically
Ans: C
In Q10, more than one options are correct, Choose all the correct options:
10.	Which of the following is(are) tags of HTML?
A) <a>					B) <b>
C) <image>				D) <href>
Ans: A, B, D
Q10 to Q13 are subjective answer type questions, Answer them briefly.
11.	What is the main difference between a web scraper and a web crawler?
Ans:
Webscrapping:- 
Web scraping is basically extracting data from websites in an automated manner. 
It is automated because it uses bots to scrape the information or content from websites. 
It’s a programmatic analysis of a web page to download information from it.
Data scraping involves locating data and then extracting it. It does not copy and paste but directly fetches the data in a precise and accurate manner. It does not limit itself to the web; data can be scraped virtually from anywhere it is stored. It does not have to be from the Internet. It is about data and not where it is stored.

Web Crawling:-
The term crawling comes from the way a spider would crawl. That’s why a web crawler is also sometimes called a spider. It’s basically an internet bot that systematically browses (read crawls) the World Wide Web, usually for the purpose of web indexing. 
It is used for indexing the information on the page using bots also known as crawlers.

It involves looking at a page in its entirety and indexing it, including its last letter and dot on the page, in the quest for information. 
Crawling through every nook and crevice of the World Wide Web, the spider locates and retrieves the information lying in the deeper layers. Web crawlers or bots navigate through heaps of data and information and procure whatever is relevant for your project.

Example of Web crawling 
What Google, Yahoo or Binge does is a straightforward example of web scraping.
These search engines crawl web pages and use the information for indexing the web pages. 


12.	What is ‘robots.txt’ file? What is the use of ‘robots.txt’ file?
Ans:-
A robots.txt is a file with set of instructions for bots or crawlers. This file is included in the source file of websites. It is a text file that tells which parts of website can be crawled and which part cannot be, it also includes a link to sitemap (can consider as directory of website) of the website. 
By using a robots.txt individual files in a directories, complete directories, sub directories can be excluded from crawling. The robots.txt data is stored in the root of the domain. It is the first document that is accessed by a bot when it visits a website. The bots of the biggest search engines such as Google and Bing follow the instructions. Otherwise there is no guarantee that a bot will adhere to the robots.txt requirements. In short, Robots.txt helps to control the crawling of search engine bots. Robots.txt is very helpful for SEO purpose.

13.	What are static and dynamic web pages?
Ans:- 
A static web page (sometimes called a flat page or a stationary page) is a web page that is delivered to the user's web browser exactly as stored, in contrast to dynamic web pages which are generated by a web application.
Static web pages are created through HTML language whereas dynamic web pages are created by the utilization of PHP, JavaScript and Actionscript languages

Dynamic Web Pages are written in languages such as CGI, AJAX, ASP, ASP.NET, etc. In dynamic web pages, the Content of pages is different for different visitors. It takes more time to load than the static web page.

Q14 and Q15 are programming practice questions. Solve it using JUPYTER NOTEBOOK and paste the solution in your answer sheets.
14.	Write a python program to check whether a webpage contains a title or not.
Ans: Please find inside Worksheet_2 folder
15.	Write a python program to access the search bar and search button on images.google.com.
Ans: Please find inside Worksheet_2 folder